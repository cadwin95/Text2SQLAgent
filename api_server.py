#!/usr/bin/env python3
"""
ğŸš€ í†µí•© AI Assistant API ì„œë²„ v3.0
====================================

ğŸ“‹ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Frontend (Next.js)                          â”‚
â”‚                    http://localhost:3000                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚ HTTP API Calls
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              í†µí•© API ì„œë²„ (FastAPI)                           â”‚
â”‚                  http://localhost:8000                          â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚ Question        â”‚    â”‚ Smart Router    â”‚                   â”‚
â”‚  â”‚ Analyzer        â”‚â”€â”€â”€â”€â–¶â”‚ - General Chat  â”‚                   â”‚
â”‚  â”‚                 â”‚    â”‚ - Data Analysis â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                   â”‚                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ OpenAI Client   â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â–¶â”‚ MCP Client      â”‚     â”‚
â”‚  â”‚ (ì¼ë°˜ ëŒ€í™”)      â”‚              â”‚   â”‚ (ë°ì´í„° ë¶„ì„)    â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                   â”‚             â”‚             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚             â”‚             â”‚
â”‚  â”‚ Fallback System â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚             â”‚
â”‚  â”‚ (MCP ì—°ê²°ì‹¤íŒ¨ì‹œ) â”‚                             â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                    â”‚ stdio/JSON-RPC
                                                    â–¼
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚         KOSIS MCP Server            â”‚
                            â”‚     mcp_servers/kosis_server/       â”‚
                            â”‚                                     â”‚
                            â”‚  ğŸ“Š Tools: í†µê³„ ë°ì´í„° ì¡°íšŒ          â”‚
                            â”‚  ğŸ“ Resources: ë©”íƒ€ë°ì´í„°           â”‚
                            â”‚  ğŸ’¬ Prompts: ë¶„ì„ í…œí”Œë¦¿            â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚ HTTPS API
                                              â–¼
                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                   â”‚   KOSIS Open API    â”‚
                                   â”‚   (í†µê³„ì²­ ê³µì‹ API)  â”‚
                                   â”‚   kosis.kr/openapi  â”‚
                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ¯ ì²˜ë¦¬ í”Œë¡œìš°:
1. Frontendì—ì„œ ì‚¬ìš©ì ì§ˆë¬¸ ì „ì†¡
2. Question Analyzerê°€ ì§ˆë¬¸ ìœ í˜• ë¶„ì„ (ì¼ë°˜/ë°ì´í„°ë¶„ì„)
3. ì¼ë°˜ ì§ˆë¬¸ â†’ OpenAI API ì§ì ‘ í˜¸ì¶œ
4. ë°ì´í„° ë¶„ì„ â†’ MCP Clientë¥¼ í†µí•´ KOSIS ì„œë²„ ì—°ê²°
5. MCP ì—°ê²° ì‹¤íŒ¨ì‹œ â†’ Fallback Systemìœ¼ë¡œ ëŒ€ì²´ ì‘ë‹µ
6. ìŠ¤íŠ¸ë¦¬ë° ë˜ëŠ” ì¼ë°˜ ì‘ë‹µìœ¼ë¡œ Frontendì— ì „ë‹¬

ğŸ”§ ì£¼ìš” íŠ¹ì§•:
- OpenAI í˜¸í™˜ API ì—”ë“œí¬ì¸íŠ¸ (/v1/chat/completions)
- ìŠ¤ë§ˆíŠ¸ ë¼ìš°íŒ…ìœ¼ë¡œ ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ ìë™ ì²˜ë¦¬
- MCP í‘œì¤€ í”„ë¡œí† ì½œ ì¤€ìˆ˜ (JSON-RPC 2.0)
- ê²¬ê³ í•œ Fallback ì‹œìŠ¤í…œ
- ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì§€ì›
"""

import os
import sys
import json
import logging
import asyncio
from typing import Dict, Any, List, Optional, AsyncGenerator
from contextlib import asynccontextmanager
from datetime import datetime

# FastAPI ê´€ë ¨
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import uvicorn

# OpenAI
import openai
from openai import OpenAI

# MCP Client (Optional)
try:
    from mcp_client.client import MCPClient, MCPServerConfig
    MCP_AVAILABLE = True
except ImportError:
    MCP_AVAILABLE = False
    logging.warning("MCP client not available - using fallback mode")

# KOSIS Fallback
from kosis_fallback import analyze_data_question

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv()

# =============================================================================
# ğŸ“‹ REQUEST/RESPONSE ëª¨ë¸
# =============================================================================

class ChatMessage(BaseModel):
    role: str  # "system", "user", "assistant"
    content: str

class ChatCompletionRequest(BaseModel):
    model: Optional[str] = "gpt-3.5-turbo"
    messages: List[ChatMessage]
    max_tokens: Optional[int] = 1000
    temperature: Optional[float] = 0.7
    stream: Optional[bool] = False

# =============================================================================
# ğŸ§  Question Analyzer
# =============================================================================

class QuestionAnalyzer:
    """ì§ˆë¬¸ ìœ í˜• ë¶„ì„ê¸°"""
    
    def __init__(self, openai_client: OpenAI):
        self.client = openai_client
        
    def analyze_question(self, question: str) -> Dict[str, Any]:
        """ì§ˆë¬¸ ë¶„ì„í•˜ì—¬ ì²˜ë¦¬ ë°©ì‹ ê²°ì •"""
        
        # ë°ì´í„° ê´€ë ¨ í‚¤ì›Œë“œ
        data_keywords = [
            "í†µê³„", "ë°ì´í„°", "ë¶„ì„", "ì¡°íšŒ", "ê²€ìƒ‰",
            "ì¸êµ¬", "ê²½ì œ", "GDP", "ë¬¼ê°€", "ê³ ìš©",
            "KOSIS", "í†µê³„ì²­", "ìë£Œ", "ìˆ˜ì¹˜",
            "ì–¼ë§ˆ", "ëª‡", "ë¹„êµ", "ì¶”ì´", "ë³€í™”"
        ]
        
        # ì¼ë°˜ ëŒ€í™” íŒ¨í„´
        general_patterns = [
            "ì•ˆë…•", "ê°ì‚¬", "ê³ ë§ˆì›Œ", "ë­ì•¼", "ëˆ„êµ¬ì•¼",
            "ì‹œê°„", "ë‚ ì§œ", "ì˜¤ëŠ˜", "ë‚´ì¼", "ì–´ì œ",
            "ë„ì›€", "ì„¤ëª…", "ì•Œë ¤ì¤˜", "ë­˜ í•  ìˆ˜ ìˆ",
            "ì–´ë–»ê²Œ", "ì™œ", "ë¬´ì—‡", "ëˆ„ê°€", "ì–¸ì œ"
        ]
        
        question_lower = question.lower()
        
        # ë°ì´í„° ê´€ë ¨ ì§ˆë¬¸ ì²´í¬
        is_data_query = any(keyword in question_lower for keyword in data_keywords)
        
        # ëª…í™•í•œ ì¼ë°˜ ì§ˆë¬¸ ì²´í¬
        is_general = any(pattern in question_lower for pattern in general_patterns)
        
        # ì‹œê°„/ë‚ ì§œ ê´€ë ¨ íŠ¹ë³„ ì²˜ë¦¬
        if any(word in question_lower for word in ["ëª‡ì‹œ", "ì‹œê°„", "ë‚ ì§œ", "ì˜¤ëŠ˜", "ì§€ê¸ˆ"]):
            return {
                "type": "general",
                "subtype": "datetime",
                "needs_mcp": False,
                "confidence": 0.95
            }
        
        # ìµœì¢… íŒë‹¨
        if is_data_query and not is_general:
            return {
                "type": "data_analysis",
                "needs_mcp": True,
                "confidence": 0.8
            }
        else:
            return {
                "type": "general",
                "needs_mcp": False,
                "confidence": 0.9
            }

# =============================================================================
# ğŸš€ FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜
# =============================================================================

# ê¸€ë¡œë²Œ ì¸ìŠ¤í„´ìŠ¤
mcp_client: Optional[MCPClient] = None
openai_client: Optional[OpenAI] = None
question_analyzer: Optional[QuestionAnalyzer] = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒëª…ì£¼ê¸° ê´€ë¦¬"""
    global mcp_client, openai_client, question_analyzer
    
    # ì‹œì‘ ì‹œ ì´ˆê¸°í™”
    logging.info("ğŸš€ í†µí•© AI Assistant ì„œë²„ ì‹œì‘")
    
    # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    question_analyzer = QuestionAnalyzer(openai_client)
    logging.info("âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    # MCP í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ì„ íƒì‚¬í•­)
    if MCP_AVAILABLE:
        try:
            mcp_client = MCPClient()
            
            # KOSIS MCP ì„œë²„ ì¶”ê°€ ì‹œë„
            kosis_config = MCPServerConfig(
                name="kosis",
                command="python",
                args=["mcp_servers/kosis_server/server.py"],
                env={"KOSIS_OPEN_API_KEY": os.getenv("KOSIS_OPEN_API_KEY", "")}
            )
            
            try:
                connected = await mcp_client.add_server(kosis_config)
                if connected:
                    logging.info("âœ… KOSIS MCP ì„œë²„ ì—°ê²° ì„±ê³µ")
                    
                    # ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ í™•ì¸
                    tools = mcp_client.list_all_tools()
                    logging.info(f"ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬: {len(tools)}ê°œ")
                    for tool in tools:
                        logging.info(f"  - {tool['name']}: {tool['description']}")
                else:
                    logging.warning("âš ï¸ KOSIS MCP ì„œë²„ ì—°ê²° ì‹¤íŒ¨ - Fallback ëª¨ë“œë¡œ ë™ì‘")
            except Exception as e:
                logging.warning(f"âš ï¸ KOSIS MCP ì„œë²„ ì—°ê²° ì˜¤ë¥˜: {e} - Fallback ëª¨ë“œë¡œ ë™ì‘")
                
        except Exception as e:
            logging.warning(f"âš ï¸ MCP í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e} - Fallback ëª¨ë“œë¡œ ë™ì‘")
            mcp_client = None
    else:
        logging.info("ğŸ“‹ MCP íŒ¨í‚¤ì§€ ì—†ìŒ - Fallback ëª¨ë“œë¡œ ë™ì‘")
        mcp_client = None
    
    yield
    
    # ì¢…ë£Œ ì‹œ ì •ë¦¬
    if mcp_client:
        await mcp_client.close_all()
    logging.info("ğŸ›‘ ì„œë²„ ì¢…ë£Œ")

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="í†µí•© AI Assistant API",
    description="General ëŒ€í™”ì™€ ë°ì´í„° ë¶„ì„ì„ ëª¨ë‘ ì§€ì›í•˜ëŠ” AI Assistant",
    version="3.0.0",
    lifespan=lifespan
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# MCP ê´€ë ¨ ë¡œê·¸ ë ˆë²¨ ì¡°ì •
logging.getLogger("MCPClient").setLevel(logging.WARNING)

# =============================================================================
# ğŸ¯ ë©”ì¸ ì—”ë“œí¬ì¸íŠ¸
# =============================================================================

@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "í†µí•© AI Assistant API v3.0",
        "capabilities": ["general_chat", "data_analysis", "kosis_integration"],
        "endpoints": {
            "chat": "/v1/chat/completions",
            "health": "/health",
            "tools": "/tools"
        }
    }

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    global mcp_client, openai_client
    
    health_status = {
        "status": "healthy",
        "openai": openai_client is not None,
        "mcp_servers": []
    }
    
    if mcp_client:
        servers = mcp_client.list_servers()
        health_status["mcp_servers"] = servers
        
    return health_status

@app.get("/tools")
async def list_tools():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡"""
    global mcp_client
    
    if not mcp_client:
        return {"tools": [], "message": "MCP client not initialized"}
        
    return {
        "tools": mcp_client.list_all_tools(),
        "resources": mcp_client.list_all_resources(),
        "prompts": mcp_client.list_all_prompts()
    }

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest):
    """OpenAI í˜¸í™˜ ì±„íŒ… API"""
    global mcp_client, openai_client, question_analyzer
    
    if not openai_client:
        raise HTTPException(status_code=503, detail="OpenAI client not initialized")
    
    try:
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ì¶œ
        user_message = None
        for message in request.messages:
            if message.role == "user":
                user_message = message.content
                break
        
        if not user_message:
            raise HTTPException(status_code=400, detail="No user message found")
        
        logger.info(f"ğŸ“¨ ì‚¬ìš©ì ì§ˆë¬¸: {user_message}")
        
        # ì§ˆë¬¸ ë¶„ì„
        analysis = question_analyzer.analyze_question(user_message)
        logger.info(f"ğŸ“Š ì§ˆë¬¸ ë¶„ì„: {analysis}")
        
        # ì²˜ë¦¬ ë°©ì‹ì— ë”°ë¼ ë¼ìš°íŒ…
        if analysis["type"] == "general":
            # ì¼ë°˜ ëŒ€í™”ëŠ” OpenAI ì§ì ‘ ì‚¬ìš©
            return await handle_general_chat(request, user_message, analysis)
        else:
            # ë°ì´í„° ë¶„ì„ì€ MCP ë„êµ¬ ì‚¬ìš©
            return await handle_data_analysis(request, user_message)
            
    except Exception as e:
        logger.error(f"âŒ ì±„íŒ… ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))

async def handle_general_chat(request: ChatCompletionRequest, user_message: str, analysis: Dict[str, Any]):
    """ì¼ë°˜ ëŒ€í™” ì²˜ë¦¬"""
    global openai_client
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
    system_prompt = """ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
    
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ìì—°ìŠ¤ëŸ½ê³  ëŒ€í™”ì²´ë¡œ ë‹µë³€í•˜ì„¸ìš”.
í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ë©°, ì´ëª¨ì§€ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ì—¬ ì¹œê·¼ê°ì„ í‘œí˜„í•˜ì„¸ìš”.

ì£¼ì˜ì‚¬í•­:
- ë°ì´í„°ë‚˜ í†µê³„ ê´€ë ¨ ì§ˆë¬¸ì´ ì•„ë‹Œ ì¼ë°˜ ëŒ€í™”ì— ì§‘ì¤‘í•˜ì„¸ìš”
- ì‹œê°„ì´ë‚˜ ë‚ ì§œë¥¼ ë¬¼ìœ¼ë©´ í˜„ì¬ ì‹œê° ê¸°ì¤€ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
- ëª¨ë¥´ëŠ” ê²ƒì€ ì†”ì§íˆ ëª¨ë¥¸ë‹¤ê³  ë‹µë³€í•˜ì„¸ìš”"""
    
    # ë‚ ì§œ/ì‹œê°„ íŠ¹ë³„ ì²˜ë¦¬
    if analysis.get("subtype") == "datetime":
        current_time = datetime.now()
        time_context = f"\n\ní˜„ì¬ ì‹œê°: {current_time.strftime('%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„')}"
        system_prompt += time_context
    
    # ë©”ì‹œì§€ êµ¬ì„±
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_message}
    ]
    
    # OpenAI API í˜¸ì¶œ
    if request.stream:
        return StreamingResponse(
            stream_openai_response(messages, request),
            media_type="text/event-stream"
        )
    else:
        response = openai_client.chat.completions.create(
            model=request.model,
            messages=messages,
            max_tokens=request.max_tokens,
            temperature=request.temperature
        )
        
        return {
            "id": f"chatcmpl-{datetime.now().timestamp():.0f}",
            "object": "chat.completion",
            "created": int(datetime.now().timestamp()),
            "model": request.model,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": response.choices[0].message.content
                },
                "finish_reason": "stop"
            }],
            "usage": {
                "prompt_tokens": response.usage.prompt_tokens,
                "completion_tokens": response.usage.completion_tokens,
                "total_tokens": response.usage.total_tokens
            }
        }

async def handle_data_analysis(request: ChatCompletionRequest, user_message: str):
    """ë°ì´í„° ë¶„ì„ ì§ˆë¬¸ ì²˜ë¦¬"""
    global mcp_client, openai_client
    
    # MCP ì„œë²„ ì—°ê²° ìƒíƒœ í™•ì¸
    mcp_available = False
    if mcp_client:
        servers = mcp_client.list_servers()
        mcp_available = any(server.get("connected", False) for server in servers)
    
    logger.info(f"ğŸ”Œ MCP ì„œë²„ ì—°ê²° ìƒíƒœ: {mcp_available}")
    
    # MCPê°€ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°ì™€ ë¶ˆê°€ëŠ¥í•œ ê²½ìš° ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
    if mcp_available:
        system_prompt = """ë‹¹ì‹ ì€ í•œêµ­ í†µê³„ì²­(KOSIS) ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬:
1. search_kosis: KOSIS ë°ì´í„° ê²€ìƒ‰
2. fetch_kosis_data: íŠ¹ì • í†µê³„í‘œ ë°ì´í„° ì¡°íšŒ
3. get_stat_list: í†µê³„ ëª©ë¡ ì¡°íšŒ

ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ê³ , 
ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  í†µì°°ë ¥ ìˆëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."""
    else:
        system_prompt = """ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

í˜„ì¬ KOSIS ë°ì´í„°ë² ì´ìŠ¤ì— ì§ì ‘ ì—°ê²°í•  ìˆ˜ ì—†ì§€ë§Œ, 
í•œêµ­ í†µê³„ì²­ ë°ì´í„°ì— ëŒ€í•œ ì¼ë°˜ì ì¸ ì •ë³´ì™€ ë¶„ì„ ë°©ë²•ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë‹¤ìŒê³¼ ê°™ì€ ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”:
1. í•´ë‹¹ ì£¼ì œì˜ ì¼ë°˜ì ì¸ í†µê³„ í˜„í™©
2. ë°ì´í„° ìˆ˜ì§‘ ë°©ë²•ê³¼ ì¶œì²˜
3. ë¶„ì„ ì‹œ ê³ ë ¤í•´ì•¼ í•  ìš”ì†Œë“¤
4. íŠ¸ë Œë“œ ë° ì¸ì‚¬ì´íŠ¸

ë‹µë³€ ë§ˆì§€ë§‰ì— "ì‹¤ì‹œê°„ ë°ì´í„°ëŠ” KOSIS(kosis.kr)ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."ë¥¼ ì¶”ê°€í•˜ì„¸ìš”."""
    
    try:
        if mcp_available:
            # MCP ì„œë²„ê°€ ì—°ê²°ëœ ê²½ìš° ë„êµ¬ í˜¸ì¶œ ì‹œë„
            plan_messages = [
                {"role": "system", "content": """ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ê³„íšì„ ìˆ˜ë¦½í•˜ëŠ” AIì…ë‹ˆë‹¤. 
ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ ì–´ë–¤ KOSIS ë„êµ¬ë¥¼ ì–´ë–»ê²Œ ì‚¬ìš©í• ì§€ JSON í˜•ì‹ìœ¼ë¡œ ê³„íšì„ ì„¸ìš°ì„¸ìš”.

ì‘ë‹µ í˜•ì‹:
{
  "steps": [
    {
      "tool": "search_kosis|fetch_kosis_data|get_stat_list",
      "params": {...},
      "purpose": "ì´ ë‹¨ê³„ì˜ ëª©ì "
    }
  ],
  "explanation": "ì „ì²´ ê³„íš ì„¤ëª…"
}"""},
                {"role": "user", "content": f"{user_message}\n\nìœ„ ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•œ ë„êµ¬ ì‚¬ìš© ê³„íšì„ JSONìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”."}
            ]
            
            plan_response = openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=plan_messages,
                max_tokens=500,
                temperature=0.1
            )
            
            # ê³„íš JSON íŒŒì‹±
            plan_text = plan_response.choices[0].message.content
            plan_json = json.loads(plan_text) if plan_text.startswith('{') else {"steps": [], "explanation": "ê³„íš ìˆ˜ë¦½ ì‹¤íŒ¨"}
            
            # MCP ë„êµ¬ ì‹¤í–‰
            results = []
            for step in plan_json.get("steps", []):
                tool_name = step.get("tool")
                params = step.get("params", {})
                
                if tool_name and tool_name in ["search_kosis", "fetch_kosis_data", "get_stat_list"]:
                    try:
                        # MCP ë„êµ¬ í˜¸ì¶œ
                        result = await mcp_client.call_tool("kosis", tool_name, params)
                        results.append({
                            "tool": tool_name,
                            "success": result.get("success", False),
                            "content": result.get("content", {})
                        })
                    except Exception as e:
                        logger.error(f"MCP tool call failed: {e}")
                        results.append({
                            "tool": tool_name,
                            "success": False,
                            "error": str(e)
                        })
            
            # ê²°ê³¼ ê¸°ë°˜ ì‘ë‹µ ìƒì„±
            analysis_messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message},
                {"role": "assistant", "content": f"ë°ì´í„° ìˆ˜ì§‘ ê²°ê³¼:\n{json.dumps(results, ensure_ascii=False, indent=2)}"},
                {"role": "user", "content": "ìœ„ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë¶„ì„ ê²°ê³¼ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”."}
            ]
            
            final_response = openai_client.chat.completions.create(
                model=request.model,
                messages=analysis_messages,
                max_tokens=request.max_tokens,
                temperature=request.temperature
            )
            
            response_content = final_response.choices[0].message.content
        else:
            # MCP ì„œë²„ê°€ ì—°ê²°ë˜ì§€ ì•Šì€ ê²½ìš° Fallback ëª¨ë“œ ì‚¬ìš©
            logger.info("ğŸ“‹ MCP ì„œë²„ ë¯¸ì—°ê²°ë¡œ Fallback ë°ì´í„° ë¶„ì„ ì‚¬ìš©")
            
            # Fallback ë°ì´í„° ì¡°íšŒ
            fallback_data = analyze_data_question(user_message)
            
            # ë°ì´í„°ë¥¼ í¬í•¨í•œ ì»¨í…ìŠ¤íŠ¸ë¡œ ë‹µë³€ ìƒì„±
            context_message = f"""
ì‚¬ìš©ì ì§ˆë¬¸: {user_message}

ì¡°íšŒëœ ë°ì´í„°:
{json.dumps(fallback_data, ensure_ascii=False, indent=2)}

ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ìƒì„¸í•˜ê³  ìœ ìš©í•œ ë¶„ì„ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
"""
            
            analysis_messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": context_message}
            ]
            
            final_response = openai_client.chat.completions.create(
                model=request.model,
                messages=analysis_messages,
                max_tokens=request.max_tokens,
                temperature=request.temperature
            )
            
            response_content = final_response.choices[0].message.content
        
    except Exception as e:
        logger.error(f"Data analysis error: {e}")
        response_content = f"""
ğŸ“Š ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.

ì§ˆë¬¸: {user_message}
ì˜¤ë¥˜: {str(e)}

ëŒ€ì‹  ì¼ë°˜ì ì¸ ì •ë³´ë¥¼ ì œê³µí•´ë“œë¦´ê²Œìš”.
KOSIS(í†µê³„ì²­)ì—ì„œëŠ” ì¸êµ¬, ê²½ì œ, ì‚¬íšŒ ë“± ë‹¤ì–‘í•œ í†µê³„ ë°ì´í„°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
êµ¬ì²´ì ì¸ ë°ì´í„° ì¡°íšŒë¥¼ ì›í•˜ì‹œë©´ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.
"""
    
    if request.stream:
        return StreamingResponse(
            stream_simple_response(response_content),
            media_type="text/event-stream"
        )
    else:
        return {
            "id": f"chatcmpl-{datetime.now().timestamp():.0f}",
            "object": "chat.completion",
            "created": int(datetime.now().timestamp()),
            "model": request.model,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": response_content
                },
                "finish_reason": "stop"
            }]
        }

# =============================================================================
# ğŸŒŠ ìŠ¤íŠ¸ë¦¬ë° í—¬í¼
# =============================================================================

async def stream_openai_response(messages: List[Dict[str, str]], request: ChatCompletionRequest):
    """OpenAI ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ"""
    global openai_client
    
    stream = openai_client.chat.completions.create(
        model=request.model,
        messages=messages,
        max_tokens=request.max_tokens,
        temperature=request.temperature,
        stream=True
    )
    
    for chunk in stream:
        if chunk.choices[0].delta.content:
            yield f"data: {json.dumps({'content': chunk.choices[0].delta.content})}\n\n"
    
    yield "data: [DONE]\n\n"

async def stream_simple_response(content: str):
    """ê°„ë‹¨í•œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ"""
    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë°
    sentences = content.split('\n')
    for sentence in sentences:
        if sentence.strip():
            yield f"data: {json.dumps({'content': sentence + '\\n'})}\n\n"
            await asyncio.sleep(0.1)
    
    yield "data: [DONE]\n\n"

# =============================================================================
# ğŸš€ ë©”ì¸ ì‹¤í–‰ë¶€
# =============================================================================

def main():
    """ì„œë²„ ì‹¤í–‰"""
    host = os.getenv("HOST", "0.0.0.0")
    port = int(os.getenv("PORT", "8000"))
    
    logger.info(f"ğŸš€ í†µí•© AI Assistant ì„œë²„ ì‹œì‘")
    logger.info(f"ğŸ“¡ ì£¼ì†Œ: http://{host}:{port}")
    logger.info(f"ğŸ”‘ OpenAI API: {'ì„¤ì •ë¨' if os.getenv('OPENAI_API_KEY') else 'ë¯¸ì„¤ì •'}")
    logger.info(f"ğŸ“Š KOSIS API: {'ì„¤ì •ë¨' if os.getenv('KOSIS_OPEN_API_KEY') else 'ë¯¸ì„¤ì •'}")
    
    uvicorn.run(
        "api_server:app",
        host=host,
        port=port,
        reload=True,
        log_level="info"
    )

if __name__ == "__main__":
    main() 