"""
ğŸ–¥ï¸ API SERVER (í†µí•© FastAPI ì„œë²„)
===================================
ì—­í• : Chainê³¼ ì—°ë™ë˜ëŠ” ì›¹ API ì„œë²„

ğŸ“– ìƒˆë¡œìš´ êµ¬ì¡°ì—ì„œì˜ ì—­í• :
- Chainê³¼ 1:1 ì—°ë™ìœ¼ë¡œ ë‹¨ìˆœí™”ëœ ì•„í‚¤í…ì²˜
- OpenAI í˜¸í™˜ API ì œê³µ (/v1/chat/completions)
- ìŠ¤íŠ¸ë¦¬ë° ë° ì¼ë°˜ ì‘ë‹µ ëª¨ë‘ ì§€ì›
- MCP Config ê¸°ë°˜ ë™ì  ë„êµ¬ ë¡œë”©

ğŸ”„ ì—°ë™:
- ../agent/chain.py: ë©”ì¸ ì²´ì¸ ì‹¤í–‰ ì—”ì§„
- ../utils/llm_client.py: LLM ì„¤ì • í†µí•©
- MCP Config: ë™ì  MCP Server ì—°ê²°

ğŸš€ í•µì‹¬ íŠ¹ì§•:
- ë‹¨ìˆœí™”ëœ ì•„í‚¤í…ì²˜: Chain í•˜ë‚˜ë§Œ ê´€ë¦¬
- í‘œì¤€ í˜¸í™˜: OpenAI API í˜¸í™˜ì„±
- ìœ ì—°í•œ ì„¤ì •: í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜ êµ¬ì„±
- í™•ì¥ì„±: MCP Configë¥¼ í†µí•œ ë„êµ¬ í™•ì¥
"""

import os
import sys
import json
import logging
import asyncio
from typing import Dict, Any, List, Optional, AsyncGenerator
from contextlib import asynccontextmanager

# FastAPI ê´€ë ¨
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import uvicorn

# ìƒìœ„ ë””ë ‰í† ë¦¬ ëª¨ë“ˆ import
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from agent.chain import Chain
from utils.llm_client import get_llm_client, get_llm_config, print_llm_status

# =============================================================================
# ğŸ“‹ REQUEST/RESPONSE ëª¨ë¸
# =============================================================================

class ChatMessage(BaseModel):
    role: str  # "system", "user", "assistant"
    content: str

class ChatCompletionRequest(BaseModel):
    model: Optional[str] = "gpt-3.5-turbo"
    messages: List[ChatMessage]
    max_tokens: Optional[int] = 1000
    temperature: Optional[float] = 0.1
    stream: Optional[bool] = False

class ChatCompletionResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[Dict[str, Any]]
    usage: Dict[str, int]

# =============================================================================
# ğŸš€ FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì •
# =============================================================================

# ê¸€ë¡œë²Œ Chain ì¸ìŠ¤í„´ìŠ¤
chain_instance: Optional[Chain] = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒëª…ì£¼ê¸° ê´€ë¦¬"""
    global chain_instance
    
    # ì‹œì‘ ì‹œ ì´ˆê¸°í™”
    logging.info("ğŸš€ API ì„œë²„ ì´ˆê¸°í™” ì‹œì‘")
    
    # LLM ì„¤ì • í™•ì¸
    print_llm_status()
    
    # Chain ì´ˆê¸°í™”
    llm_backend = os.environ.get("LLM_BACKEND", "openai")
    max_iterations = int(os.environ.get("MAX_ITERATIONS", "3"))
    
    try:
        chain_instance = Chain(llm_backend=llm_backend, max_iterations=max_iterations)
        logging.info(f"âœ… Chain ì´ˆê¸°í™” ì™„ë£Œ: {llm_backend} ë°±ì—”ë“œ, ìµœëŒ€ {max_iterations}íšŒ ë°˜ë³µ")
        
        # MCP ì‹œìŠ¤í…œ ì´ˆê¸°í™” ë° ê±´ê°• ìƒíƒœ ì²´í¬
        logging.info("ğŸ¥ MCP ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        mcp_health = await chain_instance.initialize_mcp_system()
        
        if mcp_health.get('success'):
            logging.info(f"âœ… MCP ì‹œìŠ¤í…œ ì¤€ë¹„ì™„ë£Œ: {mcp_health.get('available_tools', 0)}ê°œ ë„êµ¬ ì‚¬ìš©ê°€ëŠ¥")
        else:
            logging.warning(f"âš ï¸ MCP ì‹œìŠ¤í…œ ë¶€ë¶„ì‹¤íŒ¨: {mcp_health.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
            if mcp_health.get('fallback_needed'):
                logging.warning("ğŸš¨ í´ë°± ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤ - ì œí•œëœ ê¸°ëŠ¥ë§Œ ì‚¬ìš©ê°€ëŠ¥")
        
        # ê¶Œì¥ì‚¬í•­ ì¶œë ¥
        for recommendation in mcp_health.get('recommendations', []):
            logging.info(f"ğŸ’¡ {recommendation}")
            
    except Exception as e:
        logging.error(f"âŒ Chain ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise e
    
    yield
    
    # ì¢…ë£Œ ì‹œ ì •ë¦¬
    logging.info("ğŸ›‘ API ì„œë²„ ì¢…ë£Œ")

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="MyAgent API Server",
    description="Plan-Execute-Reflect ì²´ì¸ ê¸°ë°˜ ë°ì´í„° ë¶„ì„ API",
    version="2.0.0",
    lifespan=lifespan
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# =============================================================================
# ğŸ¯ API ì—”ë“œí¬ì¸íŠ¸ë“¤
# =============================================================================

@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "MyAgent API Server v2.0",
        "architecture": "Plan-Execute-Reflect Chain",
        "endpoints": {
            "chat": "/v1/chat/completions",
            "status": "/status",
            "health": "/health"
        }
    }

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    global chain_instance
    
    if chain_instance is None:
        raise HTTPException(status_code=503, detail="Chain not initialized")
    
    # Chain ìƒíƒœ í™•ì¸
    try:
        chain_status = chain_instance.get_chain_status()
        llm_config = get_llm_config()
        
        # MCP ê±´ê°• ìƒíƒœ ì¶”ê°€
        mcp_health = getattr(chain_instance, 'mcp_health_report', None)
        
        health_info = {
            "status": "healthy",
            "chain_ready": True,
            "llm_backend": llm_config["backend"],
            "chain_status": chain_status
        }
        
        # MCP ìƒíƒœ ì •ë³´ ì¶”ê°€
        if mcp_health:
            health_info["mcp_status"] = {
                "initialized": chain_instance.mcp_initialized,
                "total_tools": mcp_health.get('total_tools', 0),
                "available_tools": mcp_health.get('available_tools', 0),
                "success_rate": mcp_health.get('overall_success_rate', 0.0),
                "fallback_needed": mcp_health.get('fallback_needed', False),
                "recommendations": mcp_health.get('recommendations', [])
            }
        else:
            health_info["mcp_status"] = {"initialized": False}
        
        return health_info
        
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e),
            "chain_ready": False,
            "mcp_status": {"initialized": False, "error": str(e)}
        }

@app.get("/status")
async def get_status():
    """ì„œë²„ ìƒíƒœ ì¡°íšŒ"""
    global chain_instance
    
    if chain_instance is None:
        return {"error": "Chain not initialized"}
    
    try:
        return {
            "server_info": {
                "version": "2.0.0",
                "architecture": "Plan-Execute-Reflect",
                "backend": os.environ.get("LLM_BACKEND", "openai")
            },
            "chain_status": chain_instance.get_chain_status(),
            "llm_config": get_llm_config()
        }
    except Exception as e:
        return {"error": str(e)}

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest):
    """
    OpenAI í˜¸í™˜ ì±„íŒ… ì™„ë£Œ API
    
    Chainì„ í†µí•´ Plan-Execute-Reflect ì‚¬ì´í´ ì‹¤í–‰
    """
    global chain_instance
    
    if chain_instance is None:
        raise HTTPException(status_code=503, detail="Chain not initialized")
    
    try:
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ì¶œ
        user_message = None
        for message in request.messages:
            if message.role == "user":
                user_message = message.content
                break
        
        if not user_message:
            raise HTTPException(status_code=400, detail="No user message found")
        
        logger.info(f"ğŸ“¨ ì‚¬ìš©ì ì§ˆë¬¸: {user_message}")
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì—¬ë¶€ í™•ì¸
        if request.stream:
            return StreamingResponse(
                stream_chain_response(user_message, request),
                media_type="text/plain"
            )
        else:
            return await execute_chain_response(user_message, request)
    
    except Exception as e:
        logger.error(f"âŒ ì±„íŒ… ì™„ë£Œ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))

async def execute_chain_response(user_message: str, request: ChatCompletionRequest) -> ChatCompletionResponse:
    """Chain ì‹¤í–‰ ë° ì¼ë°˜ ì‘ë‹µ"""
    global chain_instance
    
    try:
        # Chain ì‹¤í–‰
        result = chain_instance.run(user_message)
        
        # ì‘ë‹µ í¬ë§·íŒ…
        if result.get('success'):
            # ì„±ê³µ ì‘ë‹µ
            response_content = format_success_response(result)
        else:
            # ì‹¤íŒ¨ ì‘ë‹µ
            response_content = f"âŒ ë¶„ì„ ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}"
        
        # OpenAI í˜•ì‹ ì‘ë‹µ ìƒì„±
        import time
        
        return ChatCompletionResponse(
            id=f"chatcmpl-{int(time.time())}",
            created=int(time.time()),
            model=request.model,
            choices=[{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": response_content
                },
                "finish_reason": "stop"
            }],
            usage={
                "prompt_tokens": len(user_message.split()),
                "completion_tokens": len(response_content.split()),
                "total_tokens": len(user_message.split()) + len(response_content.split())
            }
        )
        
    except Exception as e:
        logger.error(f"âŒ Chain ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))

async def stream_chain_response(user_message: str, request: ChatCompletionRequest) -> AsyncGenerator[str, None]:
    """Chain ì‹¤í–‰ ë° ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ"""
    global chain_instance
    
    try:
        import time
        import json
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ë©”ì‹œì§€
        yield f"data: {json.dumps({'content': 'ğŸš€ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...\\n'})}\n\n"
        await asyncio.sleep(0.1)
        
        # Chain ì‹¤í–‰ (ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ)
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(None, chain_instance.run, user_message)
        
        # ì§„í–‰ ìƒí™© ìŠ¤íŠ¸ë¦¬ë°
        chain_summary = result.get('chain_summary', {})
        iterations = chain_summary.get('total_iterations', 1)
        
        yield f"data: {json.dumps({'content': f'ğŸ“Š {iterations}íšŒ ë°˜ë³µ ì‹¤í–‰ ì™„ë£Œ\\n'})}\n\n"
        await asyncio.sleep(0.1)
        
        # ê²°ê³¼ í¬ë§·íŒ…
        if result.get('success'):
            response_content = format_success_response(result)
        else:
            response_content = f"âŒ ë¶„ì„ ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}"
        
        # ìµœì¢… ê²°ê³¼ ìŠ¤íŠ¸ë¦¬ë°
        yield f"data: {json.dumps({'content': response_content})}\n\n"
        
        # ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ
        yield "data: [DONE]\n\n"
        
    except Exception as e:
        logger.error(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")
        yield f"data: {json.dumps({'content': f'âŒ ì˜¤ë¥˜: {str(e)}'})}\n\n"
        yield "data: [DONE]\n\n"

def format_success_response(result: Dict[str, Any]) -> str:
    """ì„±ê³µ ì‘ë‹µ í¬ë§·íŒ…"""
    try:
        question = result.get('question', '')
        chain_summary = result.get('chain_summary', {})
        primary_result = result.get('result', {})
        
        # ê¸°ë³¸ ì •ë³´
        response_parts = [
            f"âœ… **ì§ˆë¬¸**: {question}",
            f"ğŸ”„ **ì‹¤í–‰ ë°©ì‹**: {chain_summary.get('execution_strategy', 'Plan-Execute-Reflect')}",
            f"ğŸ“Š **ë°˜ë³µ íšŸìˆ˜**: {chain_summary.get('total_iterations', 1)}íšŒ",
            f"â­ **í’ˆì§ˆ ì ìˆ˜**: {chain_summary.get('best_quality_score', 0):.2f}",
            ""
        ]
        
        # ê²°ê³¼ ë°ì´í„°
        result_format = result.get('result_format', 'unknown')
        
        if result_format == 'sql_query_result' and primary_result:
            # SQL ì¿¼ë¦¬ ê²°ê³¼
            response_parts.append("ğŸ“ˆ **ë¶„ì„ ê²°ê³¼**:")
            response_parts.append(f"- SQL ì¿¼ë¦¬: `{primary_result.get('query_executed', 'N/A')}`")
            response_parts.append(f"- ê²°ê³¼ í–‰ ìˆ˜: {primary_result.get('row_count', 0)}í–‰")
            
            # ìƒ˜í”Œ ë°ì´í„° í‘œì‹œ
            rows = primary_result.get('rows', [])
            if rows:
                response_parts.append("- ìƒ˜í”Œ ë°ì´í„°:")
                for i, row in enumerate(rows[:3]):  # ìµœëŒ€ 3í–‰
                    response_parts.append(f"  {i+1}. {row}")
        
        elif result_format == 'table_info' and primary_result:
            # í…Œì´ë¸” ì •ë³´
            response_parts.append("ğŸ“‹ **ìˆ˜ì§‘ëœ ë°ì´í„°**:")
            for table_name, table_info in primary_result.items():
                response_parts.append(f"- {table_name}: {table_info.get('row_count', 0)}í–‰")
        
        # ì°¨íŠ¸ ë°ì´í„°
        chart_data = result.get('chart_data')
        if chart_data:
            chart_type = chart_data.get('type', 'unknown')
            response_parts.append(f"ğŸ“Š **ì‹œê°í™”**: {chart_type} ì°¨íŠ¸ ìƒì„±ë¨")
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°
        dataframes = result.get('available_dataframes', [])
        if dataframes:
            response_parts.append(f"ğŸ’¾ **ë¡œë“œëœ ë°ì´í„°**: {', '.join(dataframes)}")
        
        return "\n".join(response_parts)
        
    except Exception as e:
        logger.error(f"âŒ ì‘ë‹µ í¬ë§·íŒ… ì˜¤ë¥˜: {e}")
        return f"âœ… ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆì§€ë§Œ ê²°ê³¼ í¬ë§·íŒ… ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

# =============================================================================
# ğŸ”§ ì¶”ê°€ ê´€ë¦¬ ì—”ë“œí¬ì¸íŠ¸ë“¤
# =============================================================================

@app.post("/reset")
async def reset_chain():
    """Chain ìƒíƒœ ì´ˆê¸°í™”"""
    global chain_instance
    
    if chain_instance is None:
        raise HTTPException(status_code=503, detail="Chain not initialized")
    
    try:
        chain_instance.reset_chain()
        return {"message": "Chain ìƒíƒœê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/execution-summary")
async def get_execution_summary():
    """ì‹¤í–‰ ìš”ì•½ ì¡°íšŒ"""
    global chain_instance
    
    if chain_instance is None:
        raise HTTPException(status_code=503, detail="Chain not initialized")
    
    try:
        return chain_instance.get_execution_summary()
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/single-execution")
async def single_execution(request: Dict[str, str]):
    """ë‹¨ì¼ ì‹¤í–‰ (ì¬ê³„íš ì—†ì´)"""
    global chain_instance
    
    if chain_instance is None:
        raise HTTPException(status_code=503, detail="Chain not initialized")
    
    question = request.get('question')
    if not question:
        raise HTTPException(status_code=400, detail="Question is required")
    
    try:
        result = chain_instance.run_single_execution(question)
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# =============================================================================
# ğŸš€ ë©”ì¸ ì‹¤í–‰ë¶€
# =============================================================================

def main():
    """ì„œë²„ ì‹¤í–‰"""
    # í™˜ê²½ë³€ìˆ˜ ì„¤ì •
    host = os.environ.get("HOST", "0.0.0.0")
    port = int(os.environ.get("PORT", "8000"))
    
    logger.info(f"ğŸš€ MyAgent API Server v2.0 ì‹œì‘")
    logger.info(f"ğŸ“¡ ì£¼ì†Œ: http://{host}:{port}")
    logger.info(f"ğŸ¤– LLM ë°±ì—”ë“œ: {os.environ.get('LLM_BACKEND', 'openai')}")
    
    # ì„œë²„ ì‹¤í–‰
    uvicorn.run(
        "api_server:app",
        host=host,
        port=port,
        reload=False,
        log_level="info"
    )

if __name__ == "__main__":
    main() 