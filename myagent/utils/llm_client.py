"""
ğŸ¤– LLM CLIENT (ê³µìš© ëª¨ë“ˆ)
========================
ì—­í• : ë‹¤ì–‘í•œ LLM ë°±ì—”ë“œë¥¼ í†µí•© ê´€ë¦¬í•˜ëŠ” ê³µìš© í´ë¼ì´ì–¸íŠ¸

ğŸ“– ì§€ì› ë°±ì—”ë“œ:
- OpenAI API (gpt-3.5-turbo, gpt-4 ë“±)
- HuggingFace Transformers
- GGUF ë¡œì»¬ ëª¨ë¸
- ê¸°íƒ€ í™•ì¥ ê°€ëŠ¥í•œ ë°±ì—”ë“œ

ğŸ¯ ì‚¬ìš©ì²˜:
- Agent Chain: ê³„íš ìˆ˜ë¦½ ë° ì¬ê³„íš
- Planner: LLM ê¸°ë°˜ ë¶„ì„ ê³„íš
- SQL Agent: ìì—°ì–´ â†’ SQL ë³€í™˜
- Server API: ì±„íŒ… ì™„ë£Œ ìš”ì²­ ì²˜ë¦¬

ğŸ”§ ì„¤ì •:
- í™˜ê²½ë³€ìˆ˜: LLM_BACKEND, OPENAI_MODEL, OPENAI_API_KEY
- ë™ì  ë°±ì—”ë“œ ì„ íƒ ë° ì„¤ì •
"""

import os
import sys
from typing import List, Dict, Any, Optional
from abc import ABC, abstractmethod

class BaseLLMClient(ABC):
    """LLM í´ë¼ì´ì–¸íŠ¸ ë² ì´ìŠ¤ í´ë˜ìŠ¤"""
    
    @abstractmethod
    def chat(self, messages: List[Dict[str, str]], **kwargs) -> str:
        """ì±„íŒ… ì™„ë£Œ ìš”ì²­"""
        pass

class OpenAIClient(BaseLLMClient):
    """OpenAI API í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self):
        try:
            import openai
            self.client = openai.OpenAI(
                api_key=os.environ.get("OPENAI_API_KEY")
            )
        except ImportError:
            raise ImportError("OpenAI íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: pip install openai")
    
    def chat(self, messages: List[Dict[str, str]], model: str = None, **kwargs) -> str:
        if model is None:
            model = os.environ.get("OPENAI_MODEL", "gpt-3.5-turbo")
        
        try:
            response = self.client.chat.completions.create(
                model=model,
                messages=messages,
                max_tokens=kwargs.get('max_tokens', 1000),
                temperature=kwargs.get('temperature', 0.1)
            )
            return response.choices[0].message.content
        except Exception as e:
            raise Exception(f"OpenAI API í˜¸ì¶œ ì˜¤ë¥˜: {e}")

class HuggingFaceClient(BaseLLMClient):
    """HuggingFace Transformers í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self):
        try:
            from transformers import pipeline
            model_name = os.environ.get("HF_MODEL", "microsoft/DialoGPT-medium")
            self.pipeline = pipeline("text-generation", model=model_name)
        except ImportError:
            raise ImportError("Transformers íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: pip install transformers")
    
    def chat(self, messages: List[Dict[str, str]], **kwargs) -> str:
        # ë©”ì‹œì§€ë¥¼ ë‹¨ì¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        prompt = "\n".join([f"{msg['role']}: {msg['content']}" for msg in messages])
        prompt += "\nassistant:"
        
        try:
            result = self.pipeline(prompt, max_length=kwargs.get('max_tokens', 500))
            return result[0]['generated_text'].split("assistant:")[-1].strip()
        except Exception as e:
            raise Exception(f"HuggingFace ëª¨ë¸ í˜¸ì¶œ ì˜¤ë¥˜: {e}")

class GGUFClient(BaseLLMClient):
    """GGUF ë¡œì»¬ ëª¨ë¸ í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self):
        try:
            from llama_cpp import Llama
            model_path = os.environ.get("GGUF_MODEL_PATH", "./models/model.gguf")
            self.llm = Llama(model_path=model_path, n_ctx=2048)
        except ImportError:
            raise ImportError("llama-cpp-python íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        except Exception as e:
            raise Exception(f"GGUF ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
    
    def chat(self, messages: List[Dict[str, str]], **kwargs) -> str:
        # ë©”ì‹œì§€ë¥¼ ChatML í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        prompt = ""
        for msg in messages:
            prompt += f"<|{msg['role']}|>\n{msg['content']}\n"
        prompt += "<|assistant|>\n"
        
        try:
            result = self.llm(prompt, max_tokens=kwargs.get('max_tokens', 500))
            return result['choices'][0]['text'].strip()
        except Exception as e:
            raise Exception(f"GGUF ëª¨ë¸ í˜¸ì¶œ ì˜¤ë¥˜: {e}")

# =============================================================================
# ğŸ¯ ê³µìš© LLM í´ë¼ì´ì–¸íŠ¸ íŒ©í† ë¦¬
# =============================================================================

def get_llm_client(backend: str = None) -> BaseLLMClient:
    """
    LLM í´ë¼ì´ì–¸íŠ¸ íŒ©í† ë¦¬ í•¨ìˆ˜
    
    Parameters:
    - backend: LLM ë°±ì—”ë“œ ì„ íƒ (openai, huggingface, gguf)
    
    Returns:
    - ì„ íƒëœ LLM í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤
    """
    if backend is None:
        backend = os.environ.get("LLM_BACKEND", "openai")
    
    backend = backend.lower()
    
    if backend == "openai":
        return OpenAIClient()
    elif backend == "huggingface" or backend == "hf":
        return HuggingFaceClient()
    elif backend == "gguf":
        return GGUFClient()
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM ë°±ì—”ë“œ: {backend}")

# =============================================================================
# ğŸ”§ LLM í´ë¼ì´ì–¸íŠ¸ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# =============================================================================

def create_chat_messages(system_prompt: str, user_prompt: str) -> List[Dict[str, str]]:
    """ì±„íŒ… ë©”ì‹œì§€ í¬ë§· ìƒì„±"""
    return [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

def extract_json_from_response(response: str) -> Optional[Dict[str, Any]]:
    """LLM ì‘ë‹µì—ì„œ JSON ì¶”ì¶œ"""
    import json
    import re
    
    # JSON ë¸”ë¡ ì°¾ê¸°
    json_match = re.search(r'```json\s*(\{.*?\})\s*```', response, re.DOTALL)
    if json_match:
        try:
            return json.loads(json_match.group(1))
        except json.JSONDecodeError:
            pass
    
    # ì§ì ‘ JSON ì°¾ê¸°
    start_idx = response.find('{')
    end_idx = response.rfind('}') + 1
    if start_idx != -1 and end_idx > start_idx:
        try:
            return json.loads(response[start_idx:end_idx])
        except json.JSONDecodeError:
            pass
    
    return None

def validate_llm_setup() -> Dict[str, Any]:
    """LLM ì„¤ì • ê²€ì¦"""
    backend = os.environ.get("LLM_BACKEND", "openai")
    status = {"backend": backend, "ready": False, "error": None}
    
    try:
        client = get_llm_client(backend)
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€
        test_response = client.chat([
            {"role": "user", "content": "Hello"}
        ], max_tokens=10)
        status["ready"] = True
        status["test_response"] = test_response[:50]
    except Exception as e:
        status["error"] = str(e)
    
    return status

# =============================================================================
# ğŸ›ï¸ í™˜ê²½ ì„¤ì • í—¬í¼
# =============================================================================

def get_llm_config() -> Dict[str, Any]:
    """í˜„ì¬ LLM ì„¤ì • ì •ë³´ ë°˜í™˜"""
    return {
        "backend": os.environ.get("LLM_BACKEND", "openai"),
        "model": os.environ.get("OPENAI_MODEL", "gpt-3.5-turbo"),
        "api_key_set": bool(os.environ.get("OPENAI_API_KEY")),
        "hf_model": os.environ.get("HF_MODEL", "microsoft/DialoGPT-medium"),
        "gguf_model_path": os.environ.get("GGUF_MODEL_PATH", "./models/model.gguf")
    }

def print_llm_status():
    """LLM ìƒíƒœ ì¶œë ¥ (ë””ë²„ê¹…ìš©)"""
    config = get_llm_config()
    print("ğŸ¤– LLM Client ìƒíƒœ:")
    print(f"  Backend: {config['backend']}")
    print(f"  Model: {config['model']}")
    print(f"  API Key: {'âœ… ì„¤ì •ë¨' if config['api_key_set'] else 'âŒ ë¯¸ì„¤ì •'}")
    
    validation = validate_llm_setup()
    print(f"  Ready: {'âœ…' if validation['ready'] else 'âŒ'}")
    if validation['error']:
        print(f"  Error: {validation['error']}")

if __name__ == "__main__":
    print_llm_status() 