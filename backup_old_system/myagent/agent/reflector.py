"""
ğŸ”„ REFLECTOR (ì¬ê³„íš ì „ë‹´)
========================
ì—­í• : ì‹¤í–‰ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ê°œì„ ëœ ì¬ê³„íšì„ ìˆ˜ë¦½

ğŸ“– ìƒˆë¡œìš´ êµ¬ì¡°ì—ì„œì˜ ì—­í• :
- Executor ì‹¤í–‰ ê²°ê³¼ ë¶„ì„ ë° í‰ê°€
- ì‹¤íŒ¨ ì›ì¸ íŒŒì•… ë° ê°œì„  ë°©ì•ˆ ë„ì¶œ
- ì‚¬ìš©ì ë§Œì¡±ë„ ê¸°ë°˜ ì¬ê³„íš ìƒì„±
- Chainì—ì„œ í˜¸ì¶œë˜ì–´ Reflection ì‚¬ì´í´ ì™„ì„±

ğŸ”„ ì—°ë™:
- ../utils/llm_client.py: LLM ê¸°ë°˜ ê²°ê³¼ ë¶„ì„
- planner.py: ê°œì„ ëœ ê³„íš ì¬ìƒì„± ìš”ì²­
- Chain: ì¬ê³„íš ê²°ê³¼ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í• ì§€ ê²°ì •
- Executor: ì‹¤í–‰ ê²°ê³¼ ë°ì´í„° ë¶„ì„

ğŸš€ í•µì‹¬ íŠ¹ì§•:
- ë‹¨ì¼ ì±…ì„: ê²°ê³¼ ë¶„ì„ ë° ì¬ê³„íšë§Œ ë‹´ë‹¹
- í•™ìŠµ ëŠ¥ë ¥: ì´ì „ ì‹¤íŒ¨ì—ì„œ ë°°ìš´ ê°œì„ ì‚¬í•­ ì ìš©
- ì ì‘ì  ì „ëµ: ë‹¤ì–‘í•œ ì‹¤íŒ¨ íŒ¨í„´ì— ë§ëŠ” ëŒ€ì‘ì±…
- ì„±ëŠ¥ ì§€í‘œ: ì •ëŸ‰ì  í‰ê°€ë¥¼ í†µí•œ ê°ê´€ì  íŒë‹¨
"""

import sys
import os
import logging
from typing import Dict, Any, List, Optional

# ìƒìœ„ ë””ë ‰í† ë¦¬ ëª¨ë“ˆ import
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.llm_client import get_llm_client, create_chat_messages, extract_json_from_response

class Reflector:
    """
    ğŸ”„ ì‹¤í–‰ ê²°ê³¼ ë¶„ì„ ë° ì¬ê³„íš ì „ë¬¸ê°€
    
    ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ì—ì„œì˜ ì—­í• :
    - ì‹¤í–‰ ê²°ê³¼ í’ˆì§ˆ í‰ê°€
    - ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„ ë° í•™ìŠµ
    - ê°œì„ ëœ ëŒ€ì•ˆ ê³„íš ì œì•ˆ
    - ì—°ì†ì ì¸ ê°œì„  ì‚¬ì´í´ ì§€ì›
    """
    
    def __init__(self, llm_backend: str = None):
        # LLM í´ë¼ì´ì–¸íŠ¸
        self.llm_client = get_llm_client(llm_backend)
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # í•™ìŠµ ë°ì´í„° (ì‹¤íŒ¨ íŒ¨í„´ ë° ê°œì„ ì‚¬í•­)
        self.failure_patterns = {}
        self.improvement_history = []
        
        # ì„±ëŠ¥ í‰ê°€ ì§€í‘œ
        self.evaluation_criteria = {
            'data_quality': {'weight': 0.3, 'min_score': 0.7},
            'execution_success': {'weight': 0.4, 'min_score': 0.8},
            'user_satisfaction': {'weight': 0.3, 'min_score': 0.6}
        }
        
        self.logger.info("[Reflector] ğŸ”„ ì¬ê³„íš ì „ë¬¸ê°€ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def analyze_execution_result(self, execution_result: Dict[str, Any], original_question: str) -> Dict[str, Any]:
        """
        ì‹¤í–‰ ê²°ê³¼ ë¶„ì„ ë° í‰ê°€
        
        Parameters:
        - execution_result: Executorì˜ ì‹¤í–‰ ê²°ê³¼
        - original_question: ì›ë˜ ì‚¬ìš©ì ì§ˆë¬¸
        
        Returns:
        - ë¶„ì„ ê²°ê³¼ ë° ì¬ê³„íš ê¶Œê³ ì‚¬í•­
        """
        try:
            self.logger.info(f"[Reflector] ğŸ” ì‹¤í–‰ ê²°ê³¼ ë¶„ì„ ì‹œì‘: {original_question}")
            
            # 1. ì •ëŸ‰ì  í‰ê°€
            quantitative_scores = self._evaluate_quantitatively(execution_result)
            
            # 2. ì •ì„±ì  í‰ê°€ (LLM ê¸°ë°˜)
            qualitative_analysis = self._evaluate_qualitatively(execution_result, original_question)
            
            # 3. ì¢…í•© í‰ê°€
            overall_assessment = self._generate_overall_assessment(
                quantitative_scores, 
                qualitative_analysis,
                execution_result
            )
            
            # 4. ê°œì„  ê¶Œê³ ì‚¬í•­ ìƒì„±
            improvement_recommendations = self._generate_improvement_recommendations(
                overall_assessment,
                execution_result,
                original_question
            )
            
            self.logger.info(f"[Reflector] âœ… ë¶„ì„ ì™„ë£Œ: ì¢…í•© ì ìˆ˜ {overall_assessment.get('total_score', 0):.2f}")
            
            return {
                'success': True,
                'analysis_result': {
                    'quantitative_scores': quantitative_scores,
                    'qualitative_analysis': qualitative_analysis,
                    'overall_assessment': overall_assessment,
                    'improvement_recommendations': improvement_recommendations
                },
                'needs_replanning': overall_assessment.get('total_score', 0) < 0.7,
                'original_question': original_question
            }
            
        except Exception as e:
            self.logger.error(f"[Reflector] âŒ ê²°ê³¼ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {
                'success': False,
                'error': str(e),
                'needs_replanning': True,
                'original_question': original_question
            }
    
    def _evaluate_quantitatively(self, execution_result: Dict[str, Any]) -> Dict[str, Any]:
        """ì •ëŸ‰ì  í‰ê°€"""
        stats = execution_result.get('stats', {})
        final_result = execution_result.get('final_result', {})
        
        # ì‹¤í–‰ ì„±ê³µë¥ 
        total_steps = stats.get('total_steps', 1)
        successful_steps = stats.get('successful_steps', 0)
        execution_success_rate = successful_steps / total_steps if total_steps > 0 else 0
        
        # ë°ì´í„° í’ˆì§ˆ í‰ê°€
        data_quality_score = self._evaluate_data_quality(final_result)
        
        # MCP ë„êµ¬ í™œìš©ë„
        mcp_calls = stats.get('mcp_calls', 0)
        mcp_utilization = min(mcp_calls / 3, 1.0)  # ìµœëŒ€ 3ê°œ í˜¸ì¶œì„ ì™„ë²½í•œ í™œìš©ìœ¼ë¡œ ê°„ì£¼
        
        # SQL ë¶„ì„ ìˆ˜í–‰ë„
        sql_queries = stats.get('sql_queries', 0)
        sql_analysis_score = min(sql_queries / 2, 1.0)  # ìµœëŒ€ 2ê°œ ì¿¼ë¦¬ë¥¼ ì™„ë²½í•œ ë¶„ì„ìœ¼ë¡œ ê°„ì£¼
        
        return {
            'execution_success_rate': execution_success_rate,
            'data_quality_score': data_quality_score,
            'mcp_utilization': mcp_utilization,
            'sql_analysis_score': sql_analysis_score,
            'total_steps_executed': total_steps,
            'successful_steps': successful_steps
        }
    
    def _evaluate_data_quality(self, final_result: Dict[str, Any]) -> float:
        """ë°ì´í„° í’ˆì§ˆ í‰ê°€"""
        try:
            data_summary = final_result.get('data_summary', {})
            total_tables = data_summary.get('total_tables', 0)
            total_rows = data_summary.get('total_rows', 0)
            
            # ê¸°ë³¸ ì ìˆ˜
            base_score = 0.5 if total_tables > 0 else 0
            
            # ë°ì´í„° ì–‘ ì ìˆ˜
            data_volume_score = min(total_rows / 100, 0.3)  # 100í–‰ì„ ê¸°ì¤€ìœ¼ë¡œ ìµœëŒ€ 0.3ì 
            
            # ë°ì´í„° ë‹¤ì–‘ì„± ì ìˆ˜ (ì¶œì²˜ì˜ ë‹¤ì–‘ì„±)
            data_sources = data_summary.get('data_sources', [])
            diversity_score = min(len(data_sources) / 2, 0.2)  # 2ê°œ ì¶œì²˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìµœëŒ€ 0.2ì 
            
            return base_score + data_volume_score + diversity_score
            
        except Exception:
            return 0.0
    
    def _evaluate_qualitatively(self, execution_result: Dict[str, Any], original_question: str) -> Dict[str, Any]:
        """ì •ì„±ì  í‰ê°€ (LLM ê¸°ë°˜)"""
        
        # ì‹¤í–‰ íˆìŠ¤í† ë¦¬ ìš”ì•½
        execution_summary = self._summarize_execution_history(execution_result.get('execution_history', []))
        
        # ìµœì¢… ê²°ê³¼ ìš”ì•½
        final_result_summary = self._summarize_final_result(execution_result.get('final_result', {}))
        
        system_prompt = """
ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ê²°ê³¼ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ğŸ¯ í‰ê°€ ëª©í‘œ: ì‹¤í–‰ ê²°ê³¼ê°€ ì‚¬ìš©ì ì§ˆë¬¸ì— ì–¼ë§ˆë‚˜ ì˜ ë‹µí–ˆëŠ”ì§€ í‰ê°€

ğŸ“‹ í‰ê°€ ê¸°ì¤€:
1. ì§ˆë¬¸ ì í•©ì„±: ê²°ê³¼ê°€ ì›ë˜ ì§ˆë¬¸ì— ì–¼ë§ˆë‚˜ ë¶€í•©í•˜ëŠ”ê°€?
2. ì™„ì„±ë„: ë¶„ì„ì´ ì¶©ë¶„íˆ ìƒì„¸í•˜ê³  ì™„ì „í•œê°€?
3. ì‹ ë¢°ì„±: ë°ì´í„°ì™€ ë¶„ì„ ë°©ë²•ì´ ì‹ ë¢°í•  ë§Œí•œê°€?
4. ì‹¤ìš©ì„±: ê²°ê³¼ê°€ ì‹¤ì œë¡œ ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ”ê°€?

ê° ê¸°ì¤€ë³„ë¡œ 1-5ì ìœ¼ë¡œ í‰ê°€í•˜ê³ , ê°œì„ ì ì„ ì œì‹œí•˜ì„¸ìš”.

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{
  "question_relevance": {
    "score": 4,
    "comment": "í‰ê°€ ì„¤ëª…"
  },
  "completeness": {
    "score": 3,
    "comment": "í‰ê°€ ì„¤ëª…"
  },
  "reliability": {
    "score": 4,
    "comment": "í‰ê°€ ì„¤ëª…"
  },
  "practicality": {
    "score": 3,
    "comment": "í‰ê°€ ì„¤ëª…"
  },
  "overall_assessment": "ì „ë°˜ì  í‰ê°€",
  "key_strengths": ["ê°•ì 1", "ê°•ì 2"],
  "improvement_areas": ["ê°œì„ ì 1", "ê°œì„ ì 2"]
}
"""
        
        user_prompt = f"""
ì›ë˜ ì§ˆë¬¸: {original_question}

ì‹¤í–‰ ê³¼ì •:
{execution_summary}

ìµœì¢… ê²°ê³¼:
{final_result_summary}

ìœ„ ê²°ê³¼ë¥¼ í‰ê°€í•´ì£¼ì„¸ìš”.
"""
        
        try:
            messages = create_chat_messages(system_prompt, user_prompt)
            response = self.llm_client.chat(messages, max_tokens=600)
            
            # JSON ì¶”ì¶œ
            evaluation_json = extract_json_from_response(response)
            
            if evaluation_json:
                return evaluation_json
            else:
                # ë°±ì—… í‰ê°€
                return self._generate_fallback_qualitative_evaluation()
                
        except Exception as e:
            self.logger.error(f"[Reflector] LLM ì •ì„±ì  í‰ê°€ ì˜¤ë¥˜: {e}")
            return self._generate_fallback_qualitative_evaluation()
    
    def _summarize_execution_history(self, execution_history: List[Dict[str, Any]]) -> str:
        """ì‹¤í–‰ íˆìŠ¤í† ë¦¬ ìš”ì•½"""
        if not execution_history:
            return "ì‹¤í–‰ íˆìŠ¤í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        summary_parts = []
        for i, step in enumerate(execution_history, 1):
            step_type = step.get('step_type', 'unknown')
            success = 'âœ…' if step.get('success') else 'âŒ'
            description = step.get('description', '')
            message = step.get('message', step.get('error', ''))
            
            summary_parts.append(f"{i}. {step_type} {success}: {description} - {message}")
        
        return "\n".join(summary_parts)
    
    def _summarize_final_result(self, final_result: Dict[str, Any]) -> str:
        """ìµœì¢… ê²°ê³¼ ìš”ì•½"""
        if not final_result:
            return "ìµœì¢… ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        summary_parts = []
        
        # ë°ì´í„° ìš”ì•½
        data_summary = final_result.get('data_summary', {})
        if data_summary:
            summary_parts.append(f"ë°ì´í„°: {data_summary.get('total_tables', 0)}ê°œ í…Œì´ë¸”, {data_summary.get('total_rows', 0)}í–‰")
        
        # SQL ê²°ê³¼ ìš”ì•½
        sql_results = final_result.get('sql_results', [])
        if sql_results:
            summary_parts.append(f"SQL ë¶„ì„: {len(sql_results)}ê°œ ì¿¼ë¦¬ ì‹¤í–‰")
        
        # ì°¨íŠ¸ ë°ì´í„°
        chart_data = final_result.get('chart_data')
        if chart_data:
            summary_parts.append(f"ì‹œê°í™”: {chart_data.get('type', 'unknown')} ì°¨íŠ¸ ìƒì„±")
        
        return "\n".join(summary_parts) if summary_parts else "êµ¬ì²´ì ì¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    def _generate_fallback_qualitative_evaluation(self) -> Dict[str, Any]:
        """ë°±ì—… ì •ì„±ì  í‰ê°€"""
        return {
            "question_relevance": {"score": 3, "comment": "í‰ê°€ ë¶ˆê°€"},
            "completeness": {"score": 3, "comment": "í‰ê°€ ë¶ˆê°€"},
            "reliability": {"score": 3, "comment": "í‰ê°€ ë¶ˆê°€"}, 
            "practicality": {"score": 3, "comment": "í‰ê°€ ë¶ˆê°€"},
            "overall_assessment": "LLM í‰ê°€ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ê¸°ë³¸ í‰ê°€ë¥¼ ì ìš©í–ˆìŠµë‹ˆë‹¤.",
            "key_strengths": ["ê¸°ë³¸ ì‹¤í–‰ ì™„ë£Œ"],
            "improvement_areas": ["LLM ê¸°ë°˜ í‰ê°€ í™œì„±í™” í•„ìš”"]
        }
    
    def _generate_overall_assessment(self, quantitative: Dict[str, Any], qualitative: Dict[str, Any], execution_result: Dict[str, Any]) -> Dict[str, Any]:
        """ì¢…í•© í‰ê°€ ìƒì„±"""
        
        # ì •ëŸ‰ì  ì ìˆ˜ ì •ê·œí™” (0-1 ë²”ìœ„)
        quant_score = (
            quantitative.get('execution_success_rate', 0) * 0.4 +
            quantitative.get('data_quality_score', 0) * 0.3 +
            quantitative.get('mcp_utilization', 0) * 0.2 +
            quantitative.get('sql_analysis_score', 0) * 0.1
        )
        
        # ì •ì„±ì  ì ìˆ˜ ì •ê·œí™” (1-5 â†’ 0-1 ë²”ìœ„)
        qual_scores = []
        for criterion in ['question_relevance', 'completeness', 'reliability', 'practicality']:
            score = qualitative.get(criterion, {}).get('score', 3)
            normalized_score = (score - 1) / 4  # 1-5 â†’ 0-1
            qual_scores.append(normalized_score)
        
        qual_score = sum(qual_scores) / len(qual_scores) if qual_scores else 0
        
        # ì¢…í•© ì ìˆ˜ (ì •ëŸ‰ 70%, ì •ì„± 30%)
        total_score = quant_score * 0.7 + qual_score * 0.3
        
        # ë“±ê¸‰ ê²°ì •
        if total_score >= 0.8:
            grade = "Excellent"
        elif total_score >= 0.6:
            grade = "Good"
        elif total_score >= 0.4:
            grade = "Fair"
        else:
            grade = "Poor"
        
        return {
            'total_score': total_score,
            'quantitative_score': quant_score,
            'qualitative_score': qual_score,
            'grade': grade,
            'meets_criteria': total_score >= 0.7,
            'score_breakdown': {
                'execution_success': quantitative.get('execution_success_rate', 0),
                'data_quality': quantitative.get('data_quality_score', 0),
                'question_relevance': qual_scores[0] if qual_scores else 0,
                'completeness': qual_scores[1] if qual_scores else 0
            }
        }
    
    def _generate_improvement_recommendations(self, assessment: Dict[str, Any], execution_result: Dict[str, Any], original_question: str) -> Dict[str, Any]:
        """ê°œì„  ê¶Œê³ ì‚¬í•­ ìƒì„±"""
        
        recommendations = {
            'priority': 'low',
            'suggested_actions': [],
            'alternative_approaches': [],
            'parameter_adjustments': {}
        }
        
        total_score = assessment.get('total_score', 0)
        
        if total_score < 0.4:
            recommendations['priority'] = 'high'
            recommendations['suggested_actions'].append("ì „ë©´ì ì¸ ê³„íš ì¬ìˆ˜ë¦½ í•„ìš”")
        elif total_score < 0.7:
            recommendations['priority'] = 'medium'
            recommendations['suggested_actions'].append("ë¶€ë¶„ì ì¸ ê°œì„  í•„ìš”")
        
        # ì‹¤í–‰ ì„±ê³µë¥ ì´ ë‚®ì€ ê²½ìš°
        if assessment.get('quantitative_score', 0) < 0.6:
            recommendations['suggested_actions'].append("MCP ë„êµ¬ í˜¸ì¶œ íŒŒë¼ë¯¸í„° ì¬ê²€í† ")
            recommendations['parameter_adjustments']['mcp_retry'] = True
        
        # ë°ì´í„° í’ˆì§ˆì´ ë‚®ì€ ê²½ìš°
        data_summary = execution_result.get('final_result', {}).get('data_summary', {})
        if data_summary.get('total_rows', 0) < 10:
            recommendations['suggested_actions'].append("ë” ë§ì€ ë°ì´í„° ìˆ˜ì§‘ í•„ìš”")
            recommendations['alternative_approaches'].append("ë‹¤ë¥¸ í†µê³„í‘œ íƒìƒ‰")
        
        # ì§ˆë¬¸ ì í•©ì„±ì´ ë‚®ì€ ê²½ìš°
        score_breakdown = assessment.get('score_breakdown', {})
        if score_breakdown.get('question_relevance', 0) < 0.5:
            recommendations['suggested_actions'].append("ì§ˆë¬¸ ë¶„ì„ ë° ê³„íš ìˆ˜ì • í•„ìš”")
            recommendations['alternative_approaches'].append("ì§ˆë¬¸ ì˜ë„ ì¬í•´ì„")
        
        return recommendations
    
    def generate_replan(self, analysis_result: Dict[str, Any], original_question: str) -> Dict[str, Any]:
        """ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¬ê³„íš ìƒì„±"""
        
        try:
            self.logger.info(f"[Reflector] ğŸ”„ ì¬ê³„íš ìƒì„±: {original_question}")
            
            improvement_recommendations = analysis_result.get('improvement_recommendations', {})
            
            # ê¸°ë³¸ ì¬ê³„íš ìƒì„± (LLM ëŒ€ì‹  ë£° ê¸°ë°˜)
            replan = self._generate_basic_replan(original_question, improvement_recommendations)
            
            self.logger.info(f"[Reflector] âœ… ì¬ê³„íš ìƒì„± ì™„ë£Œ: {len(replan.get('steps', []))}ê°œ ë‹¨ê³„")
            
            return {
                'success': True,
                'replan': replan,
                'improvement_focus': improvement_recommendations.get('priority', 'low'),
                'original_question': original_question
            }
                
        except Exception as e:
            self.logger.error(f"[Reflector] âŒ ì¬ê³„íš ìƒì„± ì˜¤ë¥˜: {e}")
            return {
                'success': False,
                'error': str(e),
                'original_question': original_question
            }
    
    def _generate_basic_replan(self, question: str, improvements: Dict[str, Any]) -> Dict[str, Any]:
        """ê¸°ë³¸ ì¬ê³„íš ìƒì„± (ë£° ê¸°ë°˜)"""
        
        priority = improvements.get('priority', 'low')
        
        if priority == 'high':
            # ì „ë©´ ì¬ìˆ˜ë¦½ - ë” í™•ì‹¤í•œ ë°ì´í„° ì†ŒìŠ¤ ì‚¬ìš©
            steps = [
                {
                    "type": "mcp_tool_call",
                    "tool_name": "search_kosis",
                    "description": f"{question} ê´€ë ¨ í†µê³„í‘œ ì¬ê²€ìƒ‰",
                    "params": {"keyword": self._extract_keyword_from_question(question)},
                    "priority": "high"
                },
                {
                    "type": "mcp_tool_call", 
                    "tool_name": "fetch_kosis_data",
                    "description": "ê°œì„ ëœ ë§¤ê°œë³€ìˆ˜ë¡œ ë°ì´í„° ì¬ìˆ˜ì§‘",
                    "params": {
                        "orgId": "101",
                        "tblId": "DT_1B040A3",
                        "prdSe": "Y",
                        "startPrdDe": "2020",
                        "endPrdDe": "2024",
                        "itmId": "T20",
                        "objL1": "00"
                    },
                    "priority": "high"
                },
                {
                    "type": "sql_analysis",
                    "description": f"ê°œì„ ëœ {question} ë¶„ì„",
                    "priority": "medium"
                }
            ]
        else:
            # ë¶€ë¶„ ê°œì„ 
            steps = [
                {
                    "type": "mcp_tool_call",
                    "tool_name": "fetch_kosis_data", 
                    "description": "ê°œì„ ëœ ë°ì´í„° ìˆ˜ì§‘",
                    "params": {
                        "orgId": "101",
                        "tblId": "DT_1B040A3",
                        "itmId": "T20",
                        "objL1": "00"
                    },
                    "priority": "high"
                },
                {
                    "type": "sql_analysis",
                    "description": f"{question} ì¬ë¶„ì„",
                    "priority": "medium"
                }
            ]
        
        return {
            "steps": steps,
            "analysis_type": "ê°œì„ ëœ ë¶„ì„",
            "improvement_focus": f"{priority} ìš°ì„ ìˆœìœ„ ê°œì„ ",
            "confidence": "medium"
        }
    
    def _extract_keyword_from_question(self, question: str) -> str:
        """ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨ ë²„ì „)"""
        keywords = ['ì¸êµ¬', 'ê²½ì œ', 'GDP', 'ë¬¼ê°€', 'ê³ ìš©', 'êµìœ¡']
        
        for keyword in keywords:
            if keyword in question:
                return keyword
        
        # ê¸°ë³¸ê°’
        return "ì¸êµ¬"
    
    def get_learning_summary(self) -> Dict[str, Any]:
        """í•™ìŠµ ë°ì´í„° ìš”ì•½"""
        return {
            'total_improvements': len(self.improvement_history),
            'recent_scores': [record.get('original_score', 0) for record in self.improvement_history[-5:]],
            'common_issues': self._analyze_common_failure_patterns(),
            'improvement_trends': self._analyze_improvement_trends()
        }
    
    def _analyze_common_failure_patterns(self) -> List[str]:
        """ê³µí†µ ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„"""
        # ê°„ë‹¨í•œ íŒ¨í„´ ë¶„ì„
        all_actions = []
        for record in self.improvement_history:
            all_actions.extend(record.get('improvement_actions', []))
        
        # ë¹ˆë„ ê¸°ë°˜ íŒ¨í„´ ì¶”ì¶œ (ê°„ë‹¨ ë²„ì „)
        action_counts = {}
        for action in all_actions:
            action_counts[action] = action_counts.get(action, 0) + 1
        
        # ìƒìœ„ 3ê°œ íŒ¨í„´ ë°˜í™˜
        sorted_actions = sorted(action_counts.items(), key=lambda x: x[1], reverse=True)
        return [action for action, count in sorted_actions[:3]]
    
    def _analyze_improvement_trends(self) -> Dict[str, Any]:
        """ê°œì„  íŠ¸ë Œë“œ ë¶„ì„"""
        if len(self.improvement_history) < 2:
            return {"trend": "insufficient_data"}
        
        recent_scores = [record.get('original_score', 0) for record in self.improvement_history[-5:]]
        
        if len(recent_scores) >= 2:
            trend = "improving" if recent_scores[-1] > recent_scores[0] else "declining"
            avg_score = sum(recent_scores) / len(recent_scores)
        else:
            trend = "stable"
            avg_score = recent_scores[0] if recent_scores else 0
        
        return {
            "trend": trend,
            "average_score": avg_score,
            "recent_improvements": len([s for s in recent_scores if s > 0.7])
        } 