#!/usr/bin/env python3
"""
ğŸ§  í•™ìŠµí˜• Reflector - ê²½í—˜ì„ í†µí•´ í•™ìŠµí•˜ëŠ” ë°˜ì„± ì „ë¬¸ê°€
========================================================
ì´ë¡ ì  ì ‘ê·¼ë²•ë“¤ì„ ì‹¤ì œë¡œ êµ¬í˜„í•œ í•™ìŠµí˜• Reflectorì…ë‹ˆë‹¤.
"""

import json
import logging
import os
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
import numpy as np

@dataclass
class ExecutionExperience:
    """ì‹¤í–‰ ê²½í—˜ ë°ì´í„°"""
    timestamp: str
    query: str
    plan: Dict[str, Any]
    execution_result: Dict[str, Any]
    quality_score: float
    success_factors: List[str]
    failure_factors: List[str]
    context: Dict[str, Any]

class LearningReflector:
    """ğŸ§  í•™ìŠµí˜• Reflector - ê²½í—˜ ê¸°ë°˜ í•™ìŠµ"""
    
    def __init__(self, llm_client, experience_db_path: str = "reflection_experiences.json"):
        self.llm_client = llm_client
        self.logger = logging.getLogger(__name__)
        
        # ê²½í—˜ ë°ì´í„°ë² ì´ìŠ¤
        self.experiences = []
        self.db_path = experience_db_path
        self.load_experiences()
        
        # í•™ìŠµ í†µê³„
        self.learning_stats = {
            'total_experiences': len(self.experiences),
            'success_patterns': {},
            'failure_patterns': {},
            'improvement_trends': []
        }
        
        self.logger.info("[LearningReflector] ğŸ§  í•™ìŠµí˜• ë°˜ì„± ì „ë¬¸ê°€ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def reflect_and_learn(self, query: str, plan: Dict[str, Any], execution_result: Dict[str, Any]) -> Dict[str, Any]:
        """ë°˜ì„±í•˜ë©´ì„œ í•™ìŠµí•˜ê¸°"""
        
        # 1. ê¸°ë³¸ í’ˆì§ˆ í‰ê°€
        quality_score = self._calculate_quality_score(execution_result)
        
        # 2. íŒ¨í„´ ë¶„ì„
        success_factors, failure_factors = self._analyze_patterns(query, plan, execution_result)
        
        # 3. ê²½í—˜ ì €ì¥
        experience = ExecutionExperience(
            timestamp=datetime.now().isoformat(),
            query=query,
            plan=plan,
            execution_result=execution_result,
            quality_score=quality_score,
            success_factors=success_factors,
            failure_factors=failure_factors,
            context=self._extract_context(query, plan)
        )
        
        self.experiences.append(experience)
        self.save_experiences()
        
        # 4. í•™ìŠµ ê¸°ë°˜ ê°œì„  ì œì•ˆ
        learning_insights = self._generate_learning_insights(experience)
        
        # 5. ê°œì„ ëœ ê³„íš ìƒì„±
        replan = self._create_improved_plan(query, plan, learning_insights)
        
        self.logger.info(f"[LearningReflector] ğŸ“ í•™ìŠµ ì™„ë£Œ: í’ˆì§ˆ {quality_score:.2f}")
        
        return {
            'quality_score': quality_score,
            'analysis': self._create_analysis(execution_result, learning_insights),
            'replan': replan,
            'learning_insights': learning_insights,
            'experience_count': len(self.experiences)
        }
    
    def _calculate_quality_score(self, execution_result: Dict[str, Any]) -> float:
        """í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        
        stats = execution_result.get('stats', {})
        total_steps = stats.get('total_steps', 0)
        successful_steps = stats.get('successful_steps', 0)
        
        if total_steps == 0:
            return 0.0
        
        # ê¸°ë³¸ ì„±ê³µë¥ 
        success_rate = successful_steps / total_steps
        
        # ë°ì´í„° ìˆ˜ì§‘ ë³´ë„ˆìŠ¤
        available_data = execution_result.get('available_data', [])
        data_bonus = len(available_data) * 0.2
        
        # MCP í˜¸ì¶œ ì„±ê³µ ë³´ë„ˆìŠ¤
        mcp_calls = stats.get('mcp_calls', 0)
        mcp_bonus = min(0.3, mcp_calls * 0.1)
        
        return min(1.0, success_rate + data_bonus + mcp_bonus)
    
    def _analyze_patterns(self, query: str, plan: Dict[str, Any], execution_result: Dict[str, Any]) -> Tuple[List[str], List[str]]:
        """íŒ¨í„´ ë¶„ì„"""
        
        success_factors = []
        failure_factors = []
        
        # ì¿¼ë¦¬ íŒ¨í„´ ë¶„ì„
        if 'KOSIS' in query:
            success_factors.append('kosis_query_pattern')
        if any(word in query for word in ['ê²€ìƒ‰', 'ì¡°íšŒ', 'ë¶„ì„']):
            success_factors.append('multi_action_query')
        
        # ì‹¤í–‰ ê²°ê³¼ íŒ¨í„´ ë¶„ì„
        execution_history = execution_result.get('execution_history', [])
        
        for step in execution_history:
            if step.get('success', False):
                step_type = step.get('step_type', '')
                if step_type == 'mcp_tool_call':
                    success_factors.append('successful_mcp_call')
                elif step_type == 'sql_analysis':
                    success_factors.append('successful_sql_analysis')
            else:
                error = step.get('error', '')
                if 'MCP' in error:
                    failure_factors.append('mcp_connection_failure')
                elif 'timeout' in error.lower():
                    failure_factors.append('timeout_failure')
                else:
                    failure_factors.append('general_execution_failure')
        
        return success_factors, failure_factors
    
    def _extract_context(self, query: str, plan: Dict[str, Any]) -> Dict[str, Any]:
        """ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        return {
            'query_length': len(query),
            'plan_complexity': len(plan.get('steps', [])),
            'has_kosis': 'KOSIS' in query,
            'has_multi_steps': any(char in query for char in ['1.', '2.', '3.']),
            'query_keywords': [word for word in ['ê²€ìƒ‰', 'ì¡°íšŒ', 'ë¶„ì„', 'ì¸êµ¬', 'í†µê³„'] if word in query]
        }
    
    def _generate_learning_insights(self, current_experience: ExecutionExperience) -> Dict[str, Any]:
        """í•™ìŠµ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        
        insights = {
            'historical_performance': self._get_historical_performance(),
            'pattern_analysis': self._analyze_historical_patterns(current_experience),
            'improvement_suggestions': [],
            'risk_factors': []
        }
        
        # ìœ ì‚¬í•œ ê³¼ê±° ê²½í—˜ ì°¾ê¸°
        similar_experiences = self._find_similar_experiences(current_experience)
        
        if similar_experiences:
            # ê³¼ê±° ì„±ê³µ ì‚¬ë¡€ ë¶„ì„
            successful_experiences = [exp for exp in similar_experiences if exp.quality_score > 0.5]
            if successful_experiences:
                best_experience = max(successful_experiences, key=lambda x: x.quality_score)
                insights['improvement_suggestions'].append(
                    f"ê³¼ê±° ìµœê³  ì„±ê³¼(ì ìˆ˜: {best_experience.quality_score:.2f})ë¥¼ ì°¸ê³ í•˜ì„¸ìš”"
                )
            
            # ê³µí†µ ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„
            common_failures = {}
            for exp in similar_experiences:
                for failure in exp.failure_factors:
                    common_failures[failure] = common_failures.get(failure, 0) + 1
            
            if common_failures:
                most_common = max(common_failures.items(), key=lambda x: x[1])
                insights['risk_factors'].append(f"ì£¼ì˜: {most_common[0]} ì‹¤íŒ¨ê°€ ë¹ˆë²ˆí•¨")
        
        return insights
    
    def _find_similar_experiences(self, current_exp: ExecutionExperience, limit: int = 5) -> List[ExecutionExperience]:
        """ìœ ì‚¬í•œ ê²½í—˜ ì°¾ê¸°"""
        
        scored_experiences = []
        current_context = current_exp.context
        
        for exp in self.experiences[:-1]:  # í˜„ì¬ ê²½í—˜ ì œì™¸
            similarity_score = self._calculate_similarity(current_context, exp.context)
            scored_experiences.append((similarity_score, exp))
        
        # ìœ ì‚¬ë„ ìˆœ ì •ë ¬
        scored_experiences.sort(key=lambda x: x[0], reverse=True)
        return [exp for _, exp in scored_experiences[:limit]]
    
    def _calculate_similarity(self, context1: Dict[str, Any], context2: Dict[str, Any]) -> float:
        """ì»¨í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        
        score = 0.0
        total_weight = 0.0
        
        # KOSIS ì¿¼ë¦¬ ì—¬ë¶€ (ê°€ì¤‘ì¹˜: 0.3)
        if context1.get('has_kosis') == context2.get('has_kosis'):
            score += 0.3
        total_weight += 0.3
        
        # ë‹¤ë‹¨ê³„ ì¿¼ë¦¬ ì—¬ë¶€ (ê°€ì¤‘ì¹˜: 0.2)
        if context1.get('has_multi_steps') == context2.get('has_multi_steps'):
            score += 0.2
        total_weight += 0.2
        
        # ê³„íš ë³µì¡ë„ ìœ ì‚¬ì„± (ê°€ì¤‘ì¹˜: 0.2)
        complexity1 = context1.get('plan_complexity', 0)
        complexity2 = context2.get('plan_complexity', 0)
        if abs(complexity1 - complexity2) <= 1:  # ë³µì¡ë„ ì°¨ì´ 1 ì´í•˜
            score += 0.2
        total_weight += 0.2
        
        # í‚¤ì›Œë“œ ìœ ì‚¬ì„± (ê°€ì¤‘ì¹˜: 0.3)
        keywords1 = set(context1.get('query_keywords', []))
        keywords2 = set(context2.get('query_keywords', []))
        if keywords1 and keywords2:
            keyword_similarity = len(keywords1.intersection(keywords2)) / len(keywords1.union(keywords2))
            score += 0.3 * keyword_similarity
        total_weight += 0.3
        
        return score / total_weight if total_weight > 0 else 0.0
    
    def _get_historical_performance(self) -> Dict[str, float]:
        """ê³¼ê±° ì„±ëŠ¥ í†µê³„"""
        
        if len(self.experiences) < 2:
            return {'average_score': 0.0, 'trend': 'insufficient_data'}
        
        scores = [exp.quality_score for exp in self.experiences]
        recent_scores = scores[-5:]  # ìµœê·¼ 5ê°œ
        
        average_score = sum(scores) / len(scores)
        recent_average = sum(recent_scores) / len(recent_scores)
        
        trend = 'improving' if recent_average > average_score else 'declining'
        
        return {
            'average_score': average_score,
            'recent_average': recent_average,
            'trend': trend,
            'total_experiences': len(self.experiences)
        }
    
    def _analyze_historical_patterns(self, current_exp: ExecutionExperience) -> Dict[str, Any]:
        """ê³¼ê±° íŒ¨í„´ ë¶„ì„"""
        
        pattern_analysis = {
            'success_pattern_frequency': {},
            'failure_pattern_frequency': {},
            'recommendations': []
        }
        
        # ì„±ê³µ/ì‹¤íŒ¨ íŒ¨í„´ ë¹ˆë„ ê³„ì‚°
        for exp in self.experiences:
            for pattern in exp.success_factors:
                pattern_analysis['success_pattern_frequency'][pattern] = \
                    pattern_analysis['success_pattern_frequency'].get(pattern, 0) + 1
            
            for pattern in exp.failure_factors:
                pattern_analysis['failure_pattern_frequency'][pattern] = \
                    pattern_analysis['failure_pattern_frequency'].get(pattern, 0) + 1
        
        # í˜„ì¬ ì‹¤íŒ¨ íŒ¨í„´ì— ëŒ€í•œ ê¶Œì¥ì‚¬í•­
        for failure in current_exp.failure_factors:
            frequency = pattern_analysis['failure_pattern_frequency'].get(failure, 0)
            if frequency > 2:  # 3íšŒ ì´ìƒ ë°œìƒí•œ ì‹¤íŒ¨
                pattern_analysis['recommendations'].append(
                    f"{failure} íŒ¨í„´ì´ {frequency}íšŒ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ëŒ€ì•ˆ ì „ëµì„ ê³ ë ¤í•˜ì„¸ìš”."
                )
        
        return pattern_analysis
    
    def _create_improved_plan(self, query: str, original_plan: Dict[str, Any], insights: Dict[str, Any]) -> Dict[str, Any]:
        """ê°œì„ ëœ ê³„íš ìƒì„±"""
        
        # ê¸°ì¡´ ê³„íš ë³µì‚¬
        improved_plan = {
            'objective': f"ê°œì„ ëœ ì‹¤í–‰: {query}",
            'strategy': 'í•™ìŠµ ê¸°ë°˜ ìµœì í™”',
            'steps': []
        }
        
        # í•™ìŠµ ê¸°ë°˜ ê°œì„ ì‚¬í•­ ì ìš©
        risk_factors = insights.get('risk_factors', [])
        
        # MCP ì—°ê²° ì‹¤íŒ¨ê°€ ë¹ˆë²ˆí•œ ê²½ìš° ëŒ€ì•ˆ ì „ëµ
        if any('mcp' in risk.lower() for risk in risk_factors):
            improved_plan['steps'].append({
                'type': 'mcp_tool_call',
                'tool_name': 'search_kosis',
                'params': {'keyword': 'ì¸êµ¬'},
                'description': 'MCP ì—°ê²° ì•ˆì •ì„± ìš°ì„  í™•ì¸',
                'priority': 'high',
                'retry_strategy': 'exponential_backoff'
            })
        
        # ê¸°ë³¸ ë°ì´í„° ì¡°íšŒ ë‹¨ê³„
        improved_plan['steps'].append({
            'type': 'mcp_tool_call',
            'tool_name': 'fetch_kosis_data',
            'params': {
                'orgId': '101',
                'tblId': 'DT_1B040A3',
                'prdSe': 'Y',
                'startPrdDe': '2020',
                'endPrdDe': '2023'
            },
            'description': 'ê²€ì¦ëœ ì£¼ë¯¼ë“±ë¡ì¸êµ¬ ë°ì´í„° ì¡°íšŒ',
            'priority': 'high'
        })
        
        # SQL ë¶„ì„ ë‹¨ê³„
        improved_plan['steps'].append({
            'type': 'sql_analysis',
            'description': 'ìˆ˜ì§‘ëœ ë°ì´í„° ë¶„ì„ ë° ìš”ì•½',
            'priority': 'medium'
        })
        
        improved_plan['total_steps'] = len(improved_plan['steps'])
        improved_plan['learning_applied'] = True
        improved_plan['risk_mitigation'] = risk_factors
        
        return improved_plan
    
    def _create_analysis(self, execution_result: Dict[str, Any], insights: Dict[str, Any]) -> str:
        """ë¶„ì„ ìš”ì•½ ìƒì„±"""
        
        stats = execution_result.get('stats', {})
        total_steps = stats.get('total_steps', 0)
        successful_steps = stats.get('successful_steps', 0)
        
        analysis = f"ì‹¤í–‰ ê²°ê³¼: {successful_steps}/{total_steps} ë‹¨ê³„ ì„±ê³µ. "
        
        historical = insights.get('historical_performance', {})
        trend = historical.get('trend', 'unknown')
        
        if trend == 'improving':
            analysis += "í•™ìŠµì„ í†µí•´ ì„±ëŠ¥ì´ ê°œì„ ë˜ê³  ìˆìŠµë‹ˆë‹¤. "
        elif trend == 'declining':
            analysis += "ì„±ëŠ¥ í•˜ë½ ì¶”ì„¸ì…ë‹ˆë‹¤. ì „ëµ ì¬ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤. "
        
        experience_count = historical.get('total_experiences', 0)
        analysis += f"ì´ {experience_count}ê°œì˜ ê²½í—˜ì—ì„œ í•™ìŠµí–ˆìŠµë‹ˆë‹¤."
        
        return analysis
    
    def save_experiences(self):
        """ê²½í—˜ ì €ì¥"""
        try:
            data = [asdict(exp) for exp in self.experiences]
            with open(self.db_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            self.logger.error(f"ê²½í—˜ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def load_experiences(self):
        """ê²½í—˜ ë¡œë“œ"""
        if os.path.exists(self.db_path):
            try:
                with open(self.db_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.experiences = [ExecutionExperience(**exp) for exp in data]
                self.logger.info(f"[LearningReflector] ğŸ“š {len(self.experiences)}ê°œ ê²½í—˜ ë¡œë“œë¨")
            except Exception as e:
                self.logger.warning(f"ê²½í—˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.experiences = [] 