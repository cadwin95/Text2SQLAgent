# api_server.py
# -------------
# β οΈ **LEGACY FILE - DEPRECATED**
# μ΄ νμΌμ€ λ” μ΄μƒ μ‚¬μ©λμ§€ μ•μµλ‹λ‹¤.
# λ€μ‹  `integrated_api_server.py`λ¥Ό μ‚¬μ©ν•μ„Έμ”.
# 
# FastAPI λ“±μΌλ΅ κµ¬ν„λ λ°±μ—”λ“ API μ„λ²„ λ©”μΈ νμΌ
# - LLM/Text2SQL/κ³µκ³µAPI μ—°λ™ μ—”λ“ν¬μΈνΈ μ κ³µ (RESTful API)
# - ν”„λ΅ νΈμ—”λ“(μ±„ν… UI λ“±) λ° μ™Έλ¶€ μ„λΉ„μ¤μ™€μ ν†µμ‹  λ‹΄λ‹Ή
# - λ‹¤μ–‘ν• LLM λ°±μ—”λ“(HuggingFace, OpenAI, gguf λ“±)μ™€ μ—°λ™
# - μμ—°μ–΄ μ§μβ†’LLMβ†’νλΌλ―Έν„° μ¶”μ¶β†’κ³µκ³µAPIβ†’κ²°κ³Ό λ°ν™ νμ΄ν”„λΌμΈ μ²΄μΈ μλ™ν™”
# - κ³µμ‹ κ·μΉ™/λ…μ„Έ(.cursor/rules/rl-text2sql-public-api.md) κΈ°λ° μ„¤κ³„/κµ¬ν„
# - ν™•μ¥μ„±/μ μ§€λ³΄μμ„±/ν…μ¤νΈ μ©μ΄μ„± κ³ λ ¤

# TODO (2024.06 κΈ°μ¤€, RL κΈ°λ° Text2SQL+κ³µκ³µAPI μλ™ν™” κ΄€μ )
# - LLM μ²΄μΈ μ—°λ™(κ²€μƒ‰β†’λ©”νƒ€β†’νλΌλ―Έν„°β†’μ΅°ν) μλ™ν™” λ―Έν΅
# - RL reward/μ‹¤ν–‰ κ²°κ³Ό κΈ°λ° ν”Όλ“λ°± κµ¬μ΅° λ―Έκµ¬ν„
# - κ° μ—”λ“ν¬μΈνΈλ³„ ν…μ¤νΈ/λ¬Έμ„ν™” λ³΄κ°• ν•„μ”

"""
β οΈ DEPRECATED: μ΄ νμΌμ€ λ” μ΄μƒ μ‚¬μ©λμ§€ μ•μµλ‹λ‹¤.

λ€μ‹  `integrated_api_server.py`λ¥Ό μ‚¬μ©ν•μ„Έμ”:
- λ” μ™„μ „ν• AgentChain ν†µν•©
- μ‹¤μ‹κ°„ μ¤νΈλ¦¬λ° μ§€μ›  
- DataFrame κΈ°λ° μ²λ¦¬
- KOSIS API μ—°λ™

μ‹¤ν–‰ λ°©λ²•:
python integrated_api_server.py

---

Hugging Faceμ 'Snowflake/Arctic-Text2SQL-R1-7B' λ¨λΈμ„ λ΅μ»¬μ—μ„ λ΅λ“ν•κ³ ,
FastAPIλ¥Ό μ‚¬μ©ν•μ—¬ OpenAIμ™€ νΈν™λλ” API μ—”λ“ν¬μΈνΈλ¥Ό μ κ³µν•λ” μ„λ²„μ…λ‹λ‹¤.

μ΄ μ„λ²„λ¥Ό μ‹¤ν–‰ν•λ©΄, λ΅μ»¬ λ¨Έμ‹ μ—μ„ Text-to-SQL λ¨λΈμ„ μ§μ ‘ κµ¬λ™ν•  μ μμΌλ©°,
'llm_text2sql_agent.py'μ™€ κ°™μ€ ν΄λΌμ΄μ–ΈνΈκ°€ μ΄ μ„λ²„μ— μ”μ²­μ„ λ³΄λ‚΄
μμ—°μ–΄ μ§μλ¥Ό SQLλ΅ λ³€ν™ν•  μ μμµλ‹λ‹¤.

μ‹¤ν–‰ λ°©λ²•:
1. `pip install -r requirements.txt`λ΅ λ¨λ“  ν¨ν‚¤μ§€λ¥Ό μ„¤μΉν•©λ‹λ‹¤.
2. ν„°λ―Έλ„μ—μ„ `python api_server.py`λ¥Ό μ‹¤ν–‰ν•©λ‹λ‹¤.
3. μ„λ²„κ°€ μ„±κ³µμ μΌλ΅ μ‹¤ν–‰λλ©΄, Uvicornμ΄ `http://127.0.0.1:8000`μ—μ„ μ‹¤ν–‰ μ¤‘μ„μ„ μ•λ¦½λ‹λ‹¤.
"""

import torch
import uvicorn
from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import List, Optional
import os

print("β οΈ  WARNING: μ΄ νμΌ(api_server.py)μ€ DEPRECATEDμ…λ‹λ‹¤.")
print("π€ λ€μ‹  λ‹¤μ λ…λ Ήμ–΄λ¥Ό μ‚¬μ©ν•μ„Έμ”:")
print("   python integrated_api_server.py")
print("")

# --- λ¨λΈ λ° ν† ν¬λ‚μ΄μ € λ΅λ“ ---
# MODEL_NAME = "Snowflake/Arctic-Text2SQL-R1-7B" # λ” μ΄μƒ ν•„μ” μ—†μ
LOCAL_MODEL_PATH = "./model-cache"

print(f"Loading model from local path '{LOCAL_MODEL_PATH}'...")
print("Please run 'download_model.py' first if you haven't.")

# λ΅μ»¬ λ¨λΈ κ²½λ΅κ°€ μ΅΄μ¬ν•λ”μ§€ ν™•μΈ
if not os.path.exists(LOCAL_MODEL_PATH):
    print(f"Error: Local model directory not found at '{LOCAL_MODEL_PATH}'")
    print("Please run 'python download_model.py' to download the model first.")
    exit(1)

try:
    # λ΅μ»¬ λ””λ ‰ν„°λ¦¬μ—μ„ λ¨λΈκ³Ό ν† ν¬λ‚μ΄μ €λ¥Ό λ΅λ“
    tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_PATH)
    model = AutoModelForCausalLM.from_pretrained(
        LOCAL_MODEL_PATH
    )
    model.to("cpu")
    print("Model loaded successfully from local path onto CPU.")
except Exception as e:
    print(f"Error loading model: {e}")
    print("Please ensure you have a compatible environment (GPU with CUDA recommended).")
    # λ¨λΈ λ΅λ”© μ‹¤ν¨ μ‹ μ„λ²„ μ‹¤ν–‰ μ¤‘λ‹¨
    exit(1)


# --- FastAPI μ•± μ„¤μ • ---
app = FastAPI(
    title="Local Text-to-SQL LLM Server",
    description=f"An API server for the {LOCAL_MODEL_PATH} model, compatible with OpenAI's API format."
)

# --- OpenAI νΈν™ APIλ¥Ό μ„ν• Pydantic λ¨λΈ μ •μ ---
class ChatMessage(BaseModel):
    role: str
    content: str

class ChatCompletionRequest(BaseModel):
    model: str # OpenAI API ν•μ‹μƒ ν•„μ”ν•μ§€λ§, μ—¬κΈ°μ„λ” μ‚¬μ©λμ§€ μ•μ
    messages: List[ChatMessage]
    max_tokens: Optional[int] = 256
    temperature: Optional[float] = 0.1

class ChatCompletionResponseChoice(BaseModel):
    index: int
    message: ChatMessage

class ChatCompletionResponse(BaseModel):
    id: str = "chatcmpl-local"
    object: str = "chat.completion"
    model: str = LOCAL_MODEL_PATH
    choices: List[ChatCompletionResponseChoice]


# --- API μ—”λ“ν¬μΈνΈ μ •μ ---
@app.post("/v1/chat/completions")
async def chat_completions(request: Request):
    # μ‹¤μ  λ¨λΈ μ¶”λ΅  λ€μ‹  ν•­μƒ κ³ μ •λ μƒν” SQL λ°ν™
    return JSONResponse({
        "choices": [
            {"message": {"content": "SELECT * FROM apartments;"}}
        ]
    })

@app.get("/")
def read_root():
    return {"message": f"Welcome to the {LOCAL_MODEL_PATH} API server. Visit /docs for API documentation."}


# --- μ„λ²„ μ‹¤ν–‰ ---
if __name__ == "__main__":
    # `reload=True`λ” μ½”λ“ λ³€κ²½ μ‹ μ„λ²„λ¥Ό μλ™μΌλ΅ μ¬μ‹μ‘ν•μ—¬ κ°λ°μ— μ μ©ν•©λ‹λ‹¤.
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True) 